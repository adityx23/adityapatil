---
layout: default
title: SLAM-Based Autonomous Navigation Robot
---

# SLAM-Based Autonomous Navigation Robot

## Overview
This project involved designing and integrating a mobile robot platform capable of autonomous indoor mapping and navigation. The focus was on full-system development: mechanical design, embedded motor control, sensor integration, simulation-based validation, and ROS2-based autonomy.

A strong emphasis was placed on simulation-to-hardware transfer, using the TurtleBot3 software ecosystem to validate SLAM and navigation behavior before deployment.

---

## Hardware Platform

**Compute & Control**
- Raspberry Pi 4B running Ubuntu 22.04 and ROS2
- ESP32 used as a dedicated low-level motor controller

**Sensors**
- YDLiDAR X2 (360° 2D LiDAR)
- BNO055 IMU (orientation and angular velocity)

**Actuation**
- 2 × 500 RPM DC encoder motors
- 2 × Cytron MD10C motor drivers

**Mechanical Design**
- Custom chassis using 3D-printed plates inspired by TurtleBot3 Waffle geometry
- Plates designed for modular mounting of compute, sensors, and motor drivers
- Components mounted using standoffs and fasteners for mechanical stability and serviceability

---

## Software Stack

**Operating System & Middleware**
- Ubuntu 22.04
- ROS2

**Simulation**
- Gazebo
- TurtleBot3 simulation environment

**Navigation & Localization**
- Nav2 for path planning and navigation
- `robot_localization` for sensor fusion
- Google Cartographer (via TurtleBot3 SLAM stack)

**Autonomy**
- Basic frontier exploration algorithm for autonomous mapping

**Drivers & Integration**
- YDLiDAR SDK integrated into ROS2
- Adafruit BNO055 drivers
- Custom ESP32 firmware for motor control and encoder feedback

---

## System Architecture (High-Level)

1. LiDAR and IMU publish sensor data to ROS2 topics on the Raspberry Pi
2. SLAM generates a map and estimates robot pose
3. `robot_localization` fuses IMU and odometry
4. Nav2 computes velocity commands
5. ESP32 receives commands and executes closed-loop motor control
6. Encoder feedback is used for odometry and debugging

Simulation was used extensively to validate this pipeline before hardware integration.

---

## Key Technical Contributions

### Mechanical & Hardware Integration
- Designed and fabricated a custom mobile robot chassis using 3D-printed plates
- Integrated motors, drivers, compute, and sensors into a compact, modular platform
- Routed power and communication interfaces to simplify debugging and maintenance

### Embedded Motor Control
- Implemented motor control on ESP32 using encoder feedback
- Separated high-level planning (Raspberry Pi) from low-level actuation (ESP32)
- Enabled clean abstraction between autonomy logic and hardware execution

### Simulation-Based Validation
- Used TurtleBot3 Gazebo simulation to validate SLAM and navigation behavior
- Tuned Nav2 and localization parameters in simulation
- Tested autonomous frontier exploration for map coverage and navigation logic

### SLAM & Navigation
- Configured Cartographer-based SLAM for indoor mapping
- Integrated LiDAR and IMU data into localization pipeline
- Debugged transform consistency, sensor update rates, and command latency issues

---

## ROS2 Data Flow and Coordinate Frames

Understanding data flow and coordinate frames was critical for stable SLAM and navigation.

**Primary ROS2 topics:**
- `/scan` — LiDAR data from YDLiDAR X2
- `/imu/data` — orientation data from BNO055
- `/odom` — wheel odometry derived from encoder feedback
- `/cmd_vel` — velocity commands generated by Nav2

**Coordinate frame hierarchy:**
- `map → odom → base_link → base_scan`

Validating transform consistency across this tree was essential for reliable localization and path planning.

---

## Failure Modes and Debugging

Several common failure modes were encountered and resolved during development:

- **Localization drift:** Caused by timing mismatches between odometry and LiDAR updates
- **Navigation instability:** Resulting from overly aggressive control parameters
- **Transform inconsistencies:** Missing or misaligned TF links leading to incorrect pose estimates
- **Frontier exploration inefficiency:** Naive frontier selection resulting in redundant exploration paths

Issues were addressed through parameter tuning, TF validation, subsystem isolation testing, and incremental reintegration.

---

## Design Decisions

**Simulation-first development**  
Using the TurtleBot3 Gazebo simulation environment enabled rapid iteration on SLAM and navigation behavior while minimizing hardware risk.

**Separation of autonomy and actuation**  
High-level planning and localization ran on the Raspberry Pi, while low-level motor control was handled by an ESP32, improving determinism and simplifying debugging.

---

## System Validation Approach

Validation followed a staged integration strategy:
1. Validate individual sensors and drivers independently
2. Verify TF tree consistency and topic update rates
3. Test SLAM and localization in isolation
4. Integrate Nav2 and frontier exploration
5. Perform end-to-end autonomous navigation tests

This approach reduced debugging complexity and improved system robustness.

---

## Results (Qualitative)

- Achieved stable SLAM and navigation behavior in simulation
- Successfully integrated hardware components into a deployable mobile robot platform
- Demonstrated autonomous exploration using frontier-based mapping
- Established a repeatable simulation-first autonomy development workflow

---

## Lessons Learned

- SLAM performance depends heavily on integration quality, not just algorithms
- Simulation is critical for reducing risk in mobile robotics
- Clear separation of system layers simplifies debugging and iteration
- Visualization and logging are essential for diagnosing autonomy failures

---

## Tools & Technologies
- ROS2, Nav2
- Gazebo simulation
- Google Cartographer
- `robot_localization`
- Raspberry Pi 4B, ESP32
- YDLiDAR X2, BNO055
- Embedded motor control with encoders
- 3D printing for robotic chassis design

---

## Future Improvements

- Quantitative benchmarking of localization drift and path tracking error
- Improved frontier selection and recovery behaviors
- Better sensor calibration and time synchronization
- Automated regression tests for navigation stability in simulation
