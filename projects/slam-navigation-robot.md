---
layout: default
title: SLAM-Based Autonomous Navigation Robot
---

# SLAM-Based Autonomous Navigation Robot

## Overview
Designed and integrated a ROS2-based autonomous mobile robot capable of indoor mapping and navigation using LiDAR and IMU sensor fusion. The project involved full-stack robotics development, including mechanical design, embedded motor control, sensor integration, simulation validation, and deployment of a complete autonomy pipeline.

**Key contributions**
- Built full autonomy pipeline: SLAM → state estimation → planning → motor execution  
- Integrated LiDAR, IMU, and encoder feedback into ROS2 navigation stack  
- Designed and fabricated custom TurtleBot3-inspired robot chassis  
- Validated autonomy pipeline in Gazebo before hardware deployment  

---

## Hardware Platform

**Compute and Control**
- Raspberry Pi 4B running Ubuntu 22.04 and ROS2  
- ESP32 used as dedicated real-time motor controller  

**Sensors**
- YDLiDAR X2 (360° 2D LiDAR) for environment perception  
- BNO055 IMU for orientation and angular velocity  

**Actuation**
- 2 × 500 RPM DC encoder motors  
- Cytron MD10C motor drivers  

**Mechanical Design**
- Custom 3D-printed chassis inspired by TurtleBot3 Waffle geometry  
- Redesigned plates to accommodate available hardware components  
- Modular mounting using standoffs for maintainability and sensor alignment  

---

## Autonomy Software Architecture

<p align="center">
  <img src="{{ site.baseurl }}/assets/slam/slam_system_architecture.png" alt="SLAM System Architecture" width="850">
</p>
<p align="center">
  <em>Distributed autonomy architecture with ROS2 running on Raspberry Pi and real-time motor control handled by ESP32.</em>
</p>

**System architecture highlights**
- ROS2 autonomy stack runs on Raspberry Pi  
- ESP32 executes deterministic low-level motor control  
- Sensor fusion combines LiDAR, IMU, and encoder data  
- Navigation commands generated by Nav2 and executed via motor controller  

---

## Autonomy Pipeline Implementation

The complete autonomy stack consisted of four major components:

### SLAM (Mapping and Localization)
- Google Cartographer used to generate occupancy grid maps and estimate robot pose  
- LiDAR provided environment perception through `/scan` topic  

### State Estimation
- `robot_localization` EKF fused IMU and encoder odometry  
- Produced stable robot pose estimates through `/odom`  

### Navigation and Planning
- Nav2 stack generated velocity commands (`/cmd_vel`) based on goal and map  
- Frontier exploration algorithm enabled autonomous map coverage  

### Motor Execution
- ESP32 received velocity commands and executed closed-loop motor control  
- Encoder feedback provided odometry for localization and debugging  

---

## Simulation and System Validation

Simulation-first development was used to validate the autonomy pipeline prior to hardware integration.

**Simulation environment**
- Gazebo simulator with TurtleBot3 software stack  
- Full SLAM and navigation pipeline tested in simulation  

**Validation approach**
1. Verified individual sensor drivers and data publishing  
2. Validated coordinate frame consistency (`map → odom → base_link`)  
3. Tested SLAM and localization independently  
4. Integrated Nav2 planning and control pipeline  
5. Validated autonomous frontier exploration behavior  

Simulation enabled rapid iteration and reduced hardware debugging complexity.

---

## Engineering Challenges and Solutions

**Compute resource limitations**
- Running full autonomy stack on Raspberry Pi required careful optimization  
- Solution: offloaded motor control to ESP32 and optimized ROS2 node execution  

**Sensor driver compatibility**
- LiDAR and IMU drivers required adaptation for ROS2 and Ubuntu 22.04  
- Resolved SDK and dependency conflicts through rebuilding and configuration  

**Localization architecture selection**
- Evaluated AMCL and `robot_localization` approaches  
- Selected `robot_localization` for IMU-integrated state estimation  
- Improved robustness of pose estimation  

**Mechanical system redesign**
- Proprietary TurtleBot3 hardware unavailable  
- Redesigned chassis to fit available motors, sensors, and compute hardware  

**Autonomous exploration implementation**
- Implemented frontier exploration algorithm for autonomous map coverage  
- Tuned goal selection and navigation parameters for stable exploration  

---

## Technical Stack

**Middleware and Autonomy**
- ROS2, Nav2 navigation stack  
- Google Cartographer SLAM  
- robot_localization EKF  

**Simulation**
- Gazebo with TurtleBot3 simulation environment  

**Hardware**
- Raspberry Pi 4B, ESP32  
- YDLiDAR X2 LiDAR  
- BNO055 IMU  
- Encoder DC motors with motor drivers  

**Mechanical**
- Custom 3D-printed robot chassis  

---

## Future Improvements

- Quantitatively evaluate localization accuracy and drift  
- Improve frontier exploration efficiency and robustness  
- Implement full hardware deployment and field testing  
- Extend architecture to support multi-robot mapping and coordination  
